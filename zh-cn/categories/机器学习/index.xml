<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>æœºå™¨å­¦ä¹  on Nansen Li&#39;s Blog
ææ¥ æ£®çš„åšå®¢
</title>
        <link>https://nansenli.com/zh-cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in æœºå™¨å­¦ä¹  on Nansen Li&#39;s Blog
ææ¥ æ£®çš„åšå®¢
</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Nansen Li ğŸŒˆ ï¼ˆææ¥ æ£®ï¼‰</copyright>
        <lastBuildDate>Wed, 13 Sep 2017 10:44:00 +0000</lastBuildDate><atom:link href="https://nansenli.com/zh-cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Deepin å®‰è£… GPUç‰ˆ Tensorflow è¦ç‚¹</title>
        <link>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deepin-%E5%AE%89%E8%A3%85-gpu%E7%89%88-tensorflow-%E8%A6%81%E7%82%B9/</link>
        <pubDate>Wed, 13 Sep 2017 10:44:00 +0000</pubDate>
        
        <guid>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deepin-%E5%AE%89%E8%A3%85-gpu%E7%89%88-tensorflow-%E8%A6%81%E7%82%B9/</guid>
        <description>&lt;h2 id=&#34;å‰è¨€&#34;&gt;å‰è¨€
&lt;/h2&gt;&lt;p&gt;è™½ç„¶æœ‰å¾ˆå¤šæ•™ç¨‹éƒ½ç»™å‡ºäº†å¦‚ä½•åœ¨Ubuntuä¸Šå®‰è£…Tensorflowï¼Œä½†æ˜¯å´æ²¡æœ‰ä¸€ç¯‡æ–‡ç« è®²å¦‚ä½•åœ¨Deepinç³»ç»Ÿå®‰è£…Tensorflowï¼Œè¿™é‡Œå°†å®‰è£…è¿‡ç¨‹çš„å‡ ä¸ªè¦ç‚¹è®²è§£ä¸€ä¸‹&lt;/p&gt;
&lt;h2 id=&#34;æµç¨‹&#34;&gt;æµç¨‹
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;é¦–å…ˆä¸‹è½½Nvidiaçš„Linuxé©±åŠ¨ã€‚åœ¨å®˜ç½‘å¯ä»¥ä¸‹è½½ã€‚ç„¶åè¿è¡Œï¼Œå¦‚æœä¸èƒ½è¿è¡Œï¼Œåˆ™éœ€è¦ä¿®æ”¹é©±åŠ¨è¿è¡Œæƒé™ï¼Œä½¿ç”¨chmod u+xæ¥æ·»åŠ è¿è¡Œæƒé™ã€‚å…³äºdeepinå®‰è£…æ˜¾å¡é©±åŠ¨ï¼Œå¯ä»¥å‚è€ƒè¿™ä¸ªæ–‡ç«  &lt;a class=&#34;link&#34; href=&#34;https://wiki.deepin.org/index.php?title=%E6%98%BE%E5%8D%A1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wiki.deepin.org/index.php?title=%E6%98%BE%E5%8D%A1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;å®‰è£…Docker-ceã€‚è¿™é‡Œä¸€å®šè¦éµå¾ªå®˜æ–¹çš„å®‰è£…æ–¹æ¡ˆï¼Œåªæ­¤ä¸€ä¸ªæ–¹æ³•å¯ä»¥æ­£å¸¸å®‰è£…æœ€æ–°ç‰ˆdockerã€‚https://wiki.deepin.org/index.php?title=Docker&lt;/li&gt;
&lt;li&gt;ä¸‹è½½Nvidia-dockerã€‚åœ¨Githubçš„Nvidia-Dockeré¡µé¢æ‰¾åˆ°Ubuntuçš„å®‰è£…åŒ…å’Œå®‰è£…æ­¥éª¤ï¼Œå®‰è£…å³å¯&lt;/li&gt;
&lt;li&gt;æ¥ä¸‹æ¥è¿è¡Œ &lt;code&gt;docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu &lt;/code&gt; æ­¤æ—¶å¯ä»¥è¿›å…¥å®˜æ–¹çš„tensorflowå¸¦GPUç‰ˆæœ¬çš„é•œåƒå•¦ï¼Œæ‰“å¼€localhost:8888å¯ä»¥çœ‹åˆ°jupyterçš„é¡µé¢&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å‘&#34;&gt;å‘
&lt;/h2&gt;&lt;p&gt;åœ¨å®‰è£…Nvidiaæ˜¾å¡é©±åŠ¨çš„æ—¶å€™ï¼Œæ ¹æ®å®‰è£…è½¯ä»¶çš„æç¤ºï¼Œéœ€è¦å…³é—­Linuxçš„æ¡Œé¢ç³»ç»ŸX serverï¼Œæ­¤æ—¶å¯ä»¥é€šè¿‡tty1~6æ¥è¿›è¡Œç»ˆç«¯æ“ä½œã€‚ä½†æ˜¯å¦‚æœDeepinå·²ç»å®‰è£…äº†è‡ªå¸¦çš„Nvidiaé©±åŠ¨ï¼Œè¦åœæ­¢lightdmæœåŠ¡æ—¶ï¼Œåœ¨Deepinåœ¨å…³é—­X serverçš„åŒæ—¶ï¼Œæ˜¾ç¤ºå™¨ä¹Ÿå…³é—­äº†ï¼Œæ— æ³•æ˜¾ç¤ºtty1 ~6çš„å±å¹•ï¼Œç”±äºè¿™ä¸ªé—®é¢˜ï¼Œå¯¼è‡´Nvidiaé©±åŠ¨æ— æ³•å®‰è£…ã€‚å³ä½¿æ‰‹åŠ¨å¸è½½å®‰è£…çš„Deepinå®˜æ–¹Nvidiaé©±åŠ¨ï¼Œç»è¿‡äº†ç¹ççš„å¸è½½æŒ‡ä»¤ï¼Œåœ¨åˆ‡æ¢åˆ°tty1 ~6åï¼Œåœ¨å…³é—­å›¾å½¢ç•Œé¢çš„æ—¶å€™ï¼Œä»ç„¶ä¼šå¯¼è‡´ttyå±å¹•è¢«å…³é—­ï¼Œé»‘å±ä¸€ç‰‡ï¼Œæ— æ³•ç»§ç»­åé¢çš„é©±åŠ¨å¸è½½å’Œé‡æ–°å®‰è£…å®˜æ–¹Nvidiaé©±åŠ¨ã€‚&lt;/p&gt;
&lt;h2 id=&#34;è§£å†³æ–¹æ¡ˆ&#34;&gt;è§£å†³æ–¹æ¡ˆ
&lt;/h2&gt;&lt;p&gt;ä¸ºäº†é¿å…Nvidiaé©±åŠ¨æ— æ³•å®‰è£…ï¼Œåˆèƒ½ç®€æ´ç§‘å­¦çš„æ“ä½œï¼Œæ‰€ä»¥å¿…é¡»åœ¨ç¬¬ä¸€æ¬¡å®‰è£…Deepinç³»ç»Ÿçš„æ—¶å€™ï¼Œåœ¨æœªå®‰è£…æ˜¾å¡é©±åŠ¨çš„æƒ…å†µä¸‹å¯¹ç”µè„‘è¿›è¡Œæ“ä½œè¿›è¡Œå®‰è£…Nvidiaæ˜¾å¡å®˜æ–¹linuxé©±åŠ¨ï¼Œé¦–å…ˆæŒ‰ä¸‹å¿«æ·é”®â€œCtrl+Alt+F2â€ï¼Œè¿›å…¥tty2ï¼Œç„¶åè¾“å…¥&lt;code&gt;sudo systemctl stop lightdm&lt;/code&gt; åœæ­¢lightdmæœåŠ¡ï¼Œæ­¤æ—¶ç”µè„‘å…³é—­X serverçš„æ—¶å€™ä¸ä¼šå¯¼è‡´æ˜¾ç¤ºå™¨çš„å…³é—­ï¼Œä¹‹åè¿è¡Œ&lt;code&gt;chmod u+x NVIDIA-Linux-x86_64-352.55.run&lt;/code&gt; #èµ‹äºˆå¯æ‰§è¡Œæƒé™ ä»¥åŠ&lt;code&gt;sudo ./NVIDIA-Linux-x86_64-352.55.run&lt;/code&gt; #å®‰è£…é©±åŠ¨æ–‡ä»¶ ç„¶åé‡å¯å°±å¯ä»¥æ­£å¸¸çš„ä½¿ç”¨å®˜æ–¹é—­æºé©±åŠ¨å•¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;åç»­&#34;&gt;åç»­
&lt;/h2&gt;&lt;p&gt;ä½¿ç”¨GPUæ¥è¿è¡ŒTensorflowç¨‹åºæ˜¯éå¸¸å¿«çš„ã€‚ä¸€èˆ¬çš„ç¨‹åºå¯ä»¥æé€Ÿ10å€ä»¥ä¸Šï¼Œæœ‰äº›ç¨‹åºå¯ä»¥æé€Ÿ50åˆ°100å€çš„é€Ÿåº¦ï¼Œæ‰€ä»¥ä½¿ç”¨GPUæ¥è¿›è¡ŒTensorflowç¼–ç¨‹æ˜¯éå¸¸æœ‰å¿…è¦çš„ã€‚
å¦å¤–ï¼Œä¸çŸ¥é“æ˜¯Deepinçš„é—®é¢˜è¿˜æ˜¯Nvidia-Dcokerçš„é—®é¢˜è¿˜æ˜¯é©±åŠ¨çš„é—®é¢˜ï¼Œåœ¨ç”µè„‘å¾…æœºä¹‹åä¼šå¯¼è‡´Tensorflowé•œåƒå‡ºé”™ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒçš„æ—¶å€™ä¸è¦è®©æœºå™¨è¿›è¡Œå¾…æœºåŠ¨ä½œã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>TensorFlow ä¸€ä¸ªç®€å•çš„å…¥é—¨ç”¨ä¾‹</title>
        <link>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/tensorflow-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%85%A5%E9%97%A8%E7%94%A8%E4%BE%8B/</link>
        <pubDate>Wed, 09 Aug 2017 07:08:00 +0000</pubDate>
        
        <guid>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/tensorflow-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%85%A5%E9%97%A8%E7%94%A8%E4%BE%8B/</guid>
        <description>&lt;h2 id=&#34;æ¡ˆä¾‹&#34;&gt;æ¡ˆä¾‹
&lt;/h2&gt;&lt;p&gt;å‡å¦‚æˆ‘æœ‰ä¸€ç³»åˆ—çš„æ•°æ®x-yï¼Œx-yä¹‹é—´æ˜¯å‘ˆçº¿æ€§å…³ç³»çš„ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦ç”¨ä¸€æ¡ç›´çº¿æ‹Ÿåˆè¿™æ¡ç›´çº¿ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•åšï¼Ÿ&lt;/p&gt;
&lt;p&gt;ä¸‹é¢çš„ç¨‹åºä¸­ï¼Œtrain_xæ˜¯ä¸€ç³»åˆ—ä»-1åˆ°1ä¹‹é—´çš„æ•°å­—ï¼Œtrain_yæ˜¯xçš„ä¸¤å€åŠ 10ï¼Œç„¶åéšæœºåŠ äº†ä¸€ä¸ª0~1ä¹‹é—´çš„æ•°å­—
æ¥ä¸‹æ¥æˆ‘ä»¬å»ºç«‹æ¨¡å‹ï¼ŒXå’ŒYæ˜¯ä¸€ä¸ªå¾…åˆå§‹åŒ–çš„å¼ é‡å ä½ç¬¦ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¸­çš„XYä¼šä¸€ç›´æ”¹å˜ä¸ºtrain_xå’Œtrain_yä¸­çš„æ•°æ®ï¼Œç„¶åä¼˜åŒ–å™¨ä¼šè¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—wæ–œç‡å’Œbæˆªè·æŒ‰ç…§ä½¿è¯¯å·®å‡å°çš„æ–¹å‘å˜åŒ–ï¼Œè¿™æ ·ï¼Œwå’Œbç»è¿‡è¿­ä»£æœ€ç»ˆå°±ä½¿æ¨¡å‹æ»¡è¶³æ•°æ®äº†&lt;/p&gt;
&lt;p&gt;æ¨¡å‹å»ºç«‹å¥½äº†ä»¥åï¼Œæˆ‘ä»¬å¼€å§‹è¿è¡Œæ¨¡å‹ã€‚é¦–å…ˆæ‰“å¼€ä¸€ä¸ªä¼šè¯ï¼Œç„¶åä¸€å®šè¦è®°å¾—åˆå§‹åŒ–å…¨éƒ¨å˜é‡ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å¯¹å…¨ä½“æ•°æ®è¿›è¡Œ10æ¬¡éå†ï¼Œåœ¨æ¯ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œå°†ä¸€ä¸ªåæ ‡è¾“å…¥ï¼Œå¹¶è®¡ç®—è¯¯å·®ï¼Œç„¶ååˆ©ç”¨æ¢¯åº¦ä¸‹é™æ›´æ­£wå’Œbã€‚æœ€åï¼Œæˆ‘ä»¬è¾“å‡ºè§£å‡ºæ¥çš„wå’Œbã€‚&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import tensorflow as tf
import numpy as np

train_x = np.linspace(-1, 1, 101)
train_y = 2 * train_x + np.random.rand(train_x.shape[0])  + 10

X = tf.placeholder(&amp;#34;float&amp;#34;)
Y = tf.placeholder(&amp;#34;float&amp;#34;)
w = tf.Variable(0.0, name = &amp;#34;w&amp;#34;)
b = tf.Variable(0.0, name = &amp;#34;b&amp;#34;)
loss = tf.square(Y - tf.multiply(X,w) - b)
train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    for i in range(10):
        for x,y in zip(train_x, train_y):
            session.run(train_op, feed_dict={X:x, Y:y})
    print(&amp;#34;w: &amp;#34;, session.run(w))
    print(&amp;#34;b: &amp;#34;, session.run(b))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;è¿è¡Œæ•ˆæœï¼š
&lt;img src=&#34;http://otwwkzjm5.bkt.clouddn.com/17-8-9/99539186.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;æ•ˆæœ&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>TensorFlow Hello World å…¥é—¨</title>
        <link>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/tensorflow-hello-world-%E5%85%A5%E9%97%A8/</link>
        <pubDate>Tue, 08 Aug 2017 09:58:00 +0000</pubDate>
        
        <guid>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/tensorflow-hello-world-%E5%85%A5%E9%97%A8/</guid>
        <description>&lt;h2 id=&#34;å¿«é€Ÿå®‰è£…æ•™ç¨‹&#34;&gt;å¿«é€Ÿå®‰è£…æ•™ç¨‹
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;ä¸€ã€å®‰è£…docker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.docker-cn.com/community-edition#/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.docker-cn.com/community-edition#/download&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç„¶åé…ç½®å®˜æ–¹ä¸­å›½é•œåƒã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-74cf522af0bc0d30.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;äºŒã€tensorflowç¯å¢ƒæ­å»º&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;$ docker run -it -p 8888:8888 tensorflow/tensorflow&lt;/code&gt;
è¿è¡Œä¸Šè¿°å‘½ä»¤ä¼šè‡ªåŠ¨ä¸‹è½½tensorflowé•œåƒï¼Œå‰ææ˜¯ä»“åº“é•œåƒè®¾ç½®æˆä¸­å›½çš„é•œåƒï¼Œå¦åˆ™ä¸‹è½½å¾ˆæ…¢ã€‚è¿è¡Œå‘½ä»¤åï¼Œå‘½ä»¤è¡Œä¼šå‡ºç°ä¸€ä¸ªç½‘å€ï¼Œä¼šæç¤ºä½ æ‰“å¼€ç½‘é¡µï¼Œæ‰“å¼€è¿™ä¸ªç½‘å€ä»¥åä¼šæ˜¾ç¤ºtensorflowçš„jupyterç¼–è¾‘ç¯å¢ƒï¼Œä¹‹åæˆ‘ä»¬å°†åœ¨ç½‘é¡µä¸­è¾“å…¥æ‰€æœ‰ä»£ç ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŒ‚è½½dockerçš„æ–‡ä»¶ç›®å½•
å¦‚æœæˆ‘ä»¬éœ€è¦æ‰“å¼€æœ¬åœ°æ–‡ä»¶ï¼Œéœ€è¦æŒ‚è½½æœ¬åœ°çš„æ–‡ä»¶å¤¹åˆ°å®¹å™¨ç›®å½•ä¸­ã€‚å…³é—­å®¹å™¨ï¼Œé‡æ–°æ‰“å¼€å®¹å™¨ï¼Œä½¿ç”¨&lt;code&gt;-v ä¸»æœºç›®å½•:å®¹å™¨ç›®å½•&lt;/code&gt;æ¥è¿›è¡ŒæŒ‚è½½ã€‚
&lt;code&gt;docker run -v /Users/hahaha/tensorflow/:/notebooks -it -p 8888:8888 tensorflow/tensorflow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å…¶ä¸­/Users/hahaha/tensorflow/æ˜¯æˆ‘çš„macçš„ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ ¹ç›®å½•ä¸‹çš„notebooksæ˜¯tensorflowä¸­çš„jupyteré»˜è®¤ç¼–è¾‘ç›®å½•ã€‚&lt;/p&gt;
&lt;h2 id=&#34;è¿è¡Œhello-world-ä»£ç &#34;&gt;è¿è¡Œhello world ä»£ç 
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-37ff525dfa0133b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;
æ–°å»ºä¸€ä¸ªPython2 Jupyteræ–‡ä»¶ï¼Œåœ¨æ–‡ä»¶ä¸­è¾“å…¥å¦‚ä¸‹çš„ä»£ç ï¼Œç„¶åç‚¹å‡»æ’­æ”¾æŒ‰é’®ï¼Œæ­¤æ—¶åœ¨ä¸‹æ–¹åº”è¯¥ä¼šå‡ºç°ä¸€æ®µHello, TensorFlow!å­—ç¬¦ä¸²ã€‚è¯´æ˜ç¨‹åºè¿è¡ŒæˆåŠŸäº†ã€‚&lt;/p&gt;
&lt;h2 id=&#34;ç¨‹åºè§£é‡Š&#34;&gt;ç¨‹åºè§£é‡Š
&lt;/h2&gt;&lt;p&gt;ä»è¿™æ®µç®€å•çš„ä»£ç å¯ä»¥äº†è§£åˆ°TensorFlowçš„ä½¿ç”¨éå¸¸æ–¹ä¾¿ï¼Œé€šè¿‡Pythonæ ‡å‡†åº“çš„å½¢å¼å¯¼å…¥ï¼Œä¸éœ€è¦å¯åŠ¨é¢å¤–çš„æœåŠ¡ã€‚ç¬¬ä¸€æ¬¡æ¥è§¦TensorFlowå¯èƒ½æ¯”è¾ƒç–‘æƒ‘ï¼Œè¾“å‡ºä¸€æ®µhelloworld Pythonæœ¬èº«å¯ä»¥å®ç°ï¼Œä¸ºä»€ä¹ˆè¦ä½¿ç”¨tf.constant()å’Œtf.Session()å‘¢ï¼Ÿå…¶å®TensorFlowé€šè¿‡Graphå’ŒSessionæ¥å®šä¹‰è¿è¡Œçš„æ¨¡å‹å’Œè®­ç»ƒï¼Œè¿™åœ¨å¤æ‚çš„æ¨¡å‹å’Œåˆ†å¸ƒå¼è®­ç»ƒä¸Šæœ‰éå¸¸å¤§å¥½å¤„ã€‚
é¦–å…ˆï¼Œåœ¨TensorFlowä¸­ï¼Œæœ‰Graphå’ŒOperationè¿™ä¸¤ä¸ªæ¦‚å¿µã€‚Operationä»£è¡¨éœ€è¦è®¡ç®—çš„å†…å®¹ã€‚ä¸€ä¸ªGraphä¸­æœ‰å¾ˆå¤šOperationã€‚é€šè¿‡Sessionæ¥æ‰§è¡ŒGraphä¸­çš„Operationã€‚&lt;/p&gt;
&lt;h2 id=&#34;åŸºæœ¬ä½¿ç”¨&#34;&gt;åŸºæœ¬ä½¿ç”¨
&lt;/h2&gt;&lt;p&gt;ä½¿ç”¨ TensorFlow, ä½ å¿…é¡»æ˜ç™½ TensorFlow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨ &lt;code&gt;å›¾ (graph)&lt;/code&gt; æ¥è¡¨ç¤ºè®¡ç®—ä»»åŠ¡.&lt;/li&gt;
&lt;li&gt;åœ¨è¢«ç§°ä¹‹ä¸ºÂ &lt;code&gt;ä¼šè¯ (Session)&lt;/code&gt; çš„ &lt;code&gt;ä¸Šä¸‹æ–‡ (context)&lt;/code&gt; ä¸­æ‰§è¡Œå›¾.&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨ &lt;code&gt;tensor&lt;/code&gt; è¡¨ç¤ºæ•°æ®.&lt;/li&gt;
&lt;li&gt;é€šè¿‡Â &lt;code&gt;å˜é‡ (Variable)&lt;/code&gt; ç»´æŠ¤çŠ¶æ€.
ä½¿ç”¨ &lt;code&gt;feed&lt;/code&gt; å’Œ &lt;code&gt;fetch&lt;/code&gt; å¯ä»¥ä¸º&lt;code&gt;ä»»æ„çš„æ“ä½œ(arbitrary operation) &lt;/code&gt;èµ‹å€¼æˆ–è€…ä»å…¶ä¸­è·å–æ•°æ®.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç»¼è¿°&#34;&gt;ç»¼è¿°
&lt;/h2&gt;&lt;p&gt;TensorFlow æ˜¯ä¸€ä¸ªç¼–ç¨‹ç³»ç»Ÿ, ä½¿ç”¨å›¾æ¥è¡¨ç¤ºè®¡ç®—ä»»åŠ¡. å›¾ä¸­çš„èŠ‚ç‚¹è¢«ç§°ä¹‹ä¸ºÂ &lt;strong&gt;op&lt;/strong&gt;Â (operation çš„ç¼©å†™)ã€‚ ä¸€ä¸ª op ä½¿ç”¨ 0 ä¸ªæˆ–å¤šä¸ªÂ Tensor ï¼Œæ‰§è¡Œè®¡ç®—äº§ç”Ÿ0ä¸ªæˆ–å¤šä¸ªÂ Tensorã€‚ æ¯ä¸ª Tensor æ˜¯ä¸€ä¸ªç±»å‹åŒ–çš„å¤šç»´æ•°ç»„ã€‚ ä¾‹å¦‚ï¼Œä½ å¯ä»¥å°†ä¸€å°ç»„å›¾åƒé›†è¡¨ç¤ºä¸ºä¸€ä¸ªå››ç»´æµ®ç‚¹æ•°æ•°ç»„ï¼Œè¿™å››ä¸ªç»´åº¦åˆ†åˆ«æ˜¯Â [batch, height, width, channels]ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ä¸ª TensorFlow å›¾ &lt;strong&gt;æè¿°&lt;/strong&gt; äº†è®¡ç®—çš„è¿‡ç¨‹ã€‚ ä¸ºäº†è¿›è¡Œè®¡ç®—ï¼Œå›¾å¿…é¡»åœ¨&lt;code&gt;ä¼šè¯&lt;/code&gt;Â é‡Œè¢«å¯åŠ¨ã€‚Â &lt;code&gt;ä¼šè¯&lt;/code&gt; å°†å›¾çš„ op åˆ†å‘åˆ°è¯¸å¦‚ CPU æˆ– GPU ä¹‹ç±»çš„è®¾å¤‡ä¸Šï¼ŒåŒæ—¶æä¾›æ‰§è¡Œ op çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•æ‰§è¡Œåï¼Œ å°†äº§ç”Ÿçš„ tensor è¿”å›ã€‚ åœ¨ Python è¯­è¨€ä¸­, è¿”å›çš„ç±»å‹æ˜¯ tensor æ˜¯&lt;a class=&#34;link&#34; href=&#34;http://www.numpy.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;numpy&lt;/a&gt;ä¸­çš„&lt;code&gt;ndarray&lt;/code&gt;å¯¹è±¡ã€‚åœ¨ C å’Œ C++ è¯­è¨€ä¸­, è¿”å›çš„ tensor æ˜¯Â tensorflow::Tensor
Â å®ä¾‹ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ä½¿ç”¨Tensorflowå®ŒæˆKaggleä»»åŠ¡â€”â€”æ³°å¦å°¼å…‹å·Titanic: Machine Learning from Disaster</title>
        <link>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8tensorflow%E5%AE%8C%E6%88%90kaggle%E4%BB%BB%E5%8A%A1%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7titanic--machine-learning-from-disaster/</link>
        <pubDate>Fri, 04 Aug 2017 10:08:00 +0000</pubDate>
        
        <guid>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8tensorflow%E5%AE%8C%E6%88%90kaggle%E4%BB%BB%E5%8A%A1%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7titanic--machine-learning-from-disaster/</guid>
        <description>&lt;h2 id=&#34;å¼•å…¥å¿…è¦åº“&#34;&gt;å¼•å…¥å¿…è¦åº“
&lt;/h2&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import csv
import tensorflow as tf
import numpy as np
import random
import sys
import pandas as pd
from pandas import DataFrame

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;è¯»å–æºæ–‡ä»¶å¹¶æ‰“å°&#34;&gt;è¯»å–æºæ–‡ä»¶å¹¶æ‰“å°
&lt;/h2&gt;&lt;p&gt;åœ¨è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ¥è§¦äº†åŸºæœ¬çš„csvæ“ä½œï¼Œå¹¶æ˜¾ç¤ºç»“æœã€‚
æˆ‘ä»¬è¯»å…¥kaggleä¸Šä¸‹è½½çš„train.csvæ–‡ä»¶ï¼Œå¹¶å±•ç¤ºå†…å®¹&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;trainFilePath = &amp;#39;./train.csv&amp;#39;

trainSize = 0

def testCSV(filePath):
    with open(filePath, &amp;#39;rb&amp;#39;) as trainFile:
        global trainSize
        csvReader = csv.reader(trainFile)
        dataList = [data for data in csvReader]
        df = DataFrame(dataList[1:], columns=dataList[0])
        trainSize = len(df)
        print(df)
        print(&amp;#34;trainSize&amp;#34;, trainSize)

testCSV(trainFilePath)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;è¯»å–æºæ–‡ä»¶å¹¶æå–æ•°æ®å»ºç«‹ç¥ç»ç½‘ç»œ&#34;&gt;è¯»å–æºæ–‡ä»¶å¹¶æå–æ•°æ®ï¼Œå»ºç«‹ç¥ç»ç½‘ç»œ
&lt;/h2&gt;&lt;p&gt;åœ¨è¿™éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¯»å–æºæ–‡ä»¶ä¸­çš„æ€§åˆ«ï¼Œé˜¶çº§ï¼Œèˆ¹è´¹ä»¥åŠSibSpï¼Œç”¨äºæ‹Ÿåˆæœ€ç»ˆçš„ç”Ÿå­˜æ¦‚ç‡
ç„¶åæˆ‘ä»¬å»ºç«‹ä¸€ä¸ªæ€»å…±5å±‚ï¼Œä¸­é—´3å±‚çš„ç¥ç»ç½‘ç»œï¼Œç¥ç»å…ƒçš„ä¸ªæ•°åˆ†åˆ«æ˜¯4-10-20-10-2ã€‚
ç„¶åè¿è¡Œè¯»å–å‡½æ•°ã€‚&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def readTrainDataCSV(filePath):
    global trainData, targetData, classifier
    with open(filePath, &amp;#39;rb&amp;#39;) as trainFile:
        csvReader = csv.reader(trainFile)
        dataList = [data for data in csvReader]
        dataSize = len(dataList) - 1
        trainData = np.ndarray((dataSize, 4), dtype=np.float32)
        targetData = np.ndarray((dataSize, 1), dtype=np.int32)
        trainDataFrame = DataFrame(dataList[1:], columns=dataList[0])
        trainDataFrame_fliter = trainDataFrame.loc[:,[&amp;#39;Pclass&amp;#39;,&amp;#39;Sex&amp;#39;,&amp;#39;SibSp&amp;#39;,&amp;#39;Fare&amp;#39;,&amp;#39;Survived&amp;#39;]]
        for i in range(dataSize):
            thisData = np.array(trainDataFrame_fliter.iloc[i])
            Pclass,Sex,SibSp,Fare,Survived = thisData
            Pclass = float(Pclass)
            Sex = 0 if Sex == &amp;#39;female&amp;#39; else 1
            SibSp = float(SibSp)
            Fare = float(Fare)
            Survived = int(Survived)
            print(Pclass,Sex,SibSp,Fare,Survived)
            trainData[i,:] = [Pclass,Sex,SibSp,Fare]
            targetData[i,:] = [Survived]
            print(thisData)
        print(trainData)
        print(targetData)
        feature_columns = [tf.contrib.layers.real_valued_column(&amp;#34;&amp;#34;, dimension=4)]
        classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                              hidden_units=[10, 20, 10],
                                              n_classes=2)
#                                               model_dir=&amp;#34;/tmp/titanic_model&amp;#34;)

readTrainDataCSV(trainFilePath)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;åˆ›å»ºè¾“å…¥æ•°æ®&#34;&gt;åˆ›å»ºè¾“å…¥æ•°æ®
&lt;/h2&gt;&lt;p&gt;æˆ‘ä»¬å°†è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾åŒ…è£…æˆä¸€ä¸ªäºŒå…ƒç»„ï¼Œå¹¶è¿”å›&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def get_train_inputs():
    x = tf.constant(trainData)
    y = tf.constant(targetData)
    print(x)
    print(y)
    return x, y

get_train_inputs()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;è®­ç»ƒæ•°æ®&#34;&gt;è®­ç»ƒæ•°æ®
&lt;/h2&gt;&lt;p&gt;æˆ‘ä»¬å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def train():
    classifier.fit(input_fn=get_train_inputs, steps=2000)

train()
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;æ£€æŸ¥å‡†ç¡®åº¦&#34;&gt;æ£€æŸ¥å‡†ç¡®åº¦
&lt;/h2&gt;&lt;p&gt;æˆ‘ä»¬ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ¥æŸ¥çœ‹å‡†ç¡®åº¦ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨éªŒè¯é›†æ¥å®Œæˆè¿™ä»¶äº‹ã€‚ä½†æ˜¯ç”±äºæˆ‘ä»¬åªæ˜¯ç”¨æ¥æ¼”ç¤ºï¼Œæ‰€ä»¥å°±ç®—äº†&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;accuracy_score = classifier.evaluate(input_fn=get_train_inputs,
                                       steps=1)[&amp;#34;accuracy&amp;#34;]
print(&amp;#34;accuracy:&amp;#34;,accuracy_score)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;è¯»å…¥æµ‹è¯•é›†å¹¶è¾“å‡ºç»“æœ&#34;&gt;è¯»å…¥æµ‹è¯•é›†ï¼Œå¹¶è¾“å‡ºç»“æœ
&lt;/h2&gt;&lt;p&gt;åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è¯»å…¥kaggleä¸­çš„æ•°æ®ï¼Œå¹¶è¾“å‡ºåˆ°æ–‡ä»¶ä¸­ï¼Œæœ€ç»ˆæäº¤å®˜ç½‘&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;testFilePath = &amp;#39;./test.csv&amp;#39;

def readTestDataCSV(filePath):
    global testData, PassengerIdStart
    with open(filePath, &amp;#39;rb&amp;#39;) as testFile:
        csvReader = csv.reader(testFile)
        dataList = [data for data in csvReader]
        dataSize = len(dataList)-1
        trainDataFrame = DataFrame(dataList[1:], columns=dataList[0])
        trainDataFrame_fliter = trainDataFrame.loc[:,[&amp;#39;Pclass&amp;#39;,&amp;#39;Sex&amp;#39;,&amp;#39;SibSp&amp;#39;,&amp;#39;Fare&amp;#39;]]
        testData = np.ndarray((dataSize, 4), dtype=np.float32)
        PassengerIdStart = trainDataFrame[&amp;#39;PassengerId&amp;#39;][0]
        PassengerIdStart = int(PassengerIdStart)
        print(&amp;#39;PassengerId&amp;#39;,PassengerIdStart)
        for i in range(dataSize):
            thisData = np.array(trainDataFrame_fliter.iloc[i])
            Pclass,Sex,SibSp,Fare = thisData
            Pclass = float(Pclass)
            Sex = 0 if Sex == &amp;#39;female&amp;#39; else 1
            SibSp = float(SibSp)
            Fare = 0 if Fare==&amp;#39;&amp;#39; else float(Fare)
            print(Pclass,Sex,SibSp,Fare)
            testData[i,:] = [Pclass,Sex,SibSp,Fare]
            print(thisData)
        print(testData)
        
def testData_samples():
    return testData

readTestDataCSV(testFilePath)
predictions = list(classifier.predict(input_fn=testData_samples))
print(predictions)


with open(&amp;#39;predictions.csv&amp;#39;, &amp;#39;wb&amp;#39;) as csvfile:
    writer = csv.writer(csvfile, dialect=&amp;#39;excel&amp;#39;)
    writer.writerow([&amp;#39;PassengerId&amp;#39;,&amp;#39;Survived&amp;#39;])
    PassengerId = PassengerIdStart 
    for i in predictions:
        writer.writerow([PassengerId, i])
        PassengerId += 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;æœ€ç»ˆåœ¨åªä½¿ç”¨äº†4ä¸ªç‰¹å¾å€¼çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡æœ‰75%ã€‚æ¥ä¸‹æ¥çš„ç›®æ ‡æ˜¯å°†å…¶ä»–æ•°æ®è¿›è¡Œåˆ©ç”¨ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>æ·±åº¦å­¦ä¹ é›¶åŸºç¡€å­¦ä¹ ç¬”è®°ä¸€ï¼ˆä¼˜è¾¾å­¦åŸï¼‰</title>
        <link>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%E4%BC%98%E8%BE%BE%E5%AD%A6%E5%9F%8E/</link>
        <pubDate>Wed, 19 Jul 2017 07:42:00 +0000</pubDate>
        
        <guid>https://nansenli.com/zh-cn/post/jianshu/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%E4%BC%98%E8%BE%BE%E5%AD%A6%E5%9F%8E/</guid>
        <description>&lt;h2 id=&#34;å‰è¨€&#34;&gt;å‰è¨€
&lt;/h2&gt;&lt;p&gt;çªå‘å¥‡æƒ³æƒ³å­¦æœºå™¨å­¦ä¹ ï¼Œè¿™é‡Œæ˜¯å­¦ä¹ è¿‡ç¨‹çš„ç¬”è®°&lt;/p&gt;
&lt;h2 id=&#34;å‡†å¤‡&#34;&gt;å‡†å¤‡
&lt;/h2&gt;&lt;p&gt;æˆ‘åšäº†è¿™äº›å‡†å¤‡å·¥ä½œ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MacBook ä¸€å°ï¼Œæ­å»ºå¥½Pythonç¯å¢ƒï¼Œå®‰è£…numpyå’Œmatplotlib&lt;/li&gt;
&lt;li&gt;ä¼˜è¾¾å­¦åŸæ³¨å†Œå…è´¹çš„ã€Šæ·±åº¦å­¦ä¹ ã€‹è¯¾ç¨‹ï¼ˆGoogleåˆä½œï¼‰&lt;/li&gt;
&lt;li&gt;å»–é›ªå³°Pythonå…¥é—¨æ•™ç¨‹å­¦ä¹ &lt;/li&gt;
&lt;li&gt;èŠ±è´¹ä¸¤å¤©æ—¶é—´å¤§è‡´æµè§ˆã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å­¦ä¹ è¿™äº›çŸ¥è¯†åº”è¯¥è¶³ä»¥è¿›è¡Œæ¥ä¸‹æ¥çš„ä¼˜è¾¾å­¦åŸçš„å­¦ä¹ &lt;/p&gt;
&lt;h2 id=&#34;è¯¾ç¨‹ä¸€-ä»æœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ &#34;&gt;è¯¾ç¨‹ä¸€ ä»æœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ 
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-deb922c5a6a30e32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;å‰è¨€&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å°èŠ‚1-8ï¼Œä¸»è¦ä»‹ç»äº†æ·±åº¦å­¦ä¹ çš„å‘å±•ç°çŠ¶ç­‰ç­‰çŸ¥è¯†ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-881aa1d922b7aadf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å°èŠ‚9-12ä»‹ç»äº†softmaxæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;ç²—ç•¥æµè§ˆæœºå™¨å­¦ä¹ å®æˆ˜åï¼Œåœ¨æœºå™¨å­¦ä¹ å®æˆ˜è¿™æœ¬ä¹¦ä¸­ï¼Œå¤§è‡´ä»‹ç»äº†æœºå™¨å­¦ä¹ çš„å‡ ç§ç®—æ³•ã€‚ä»è¡¨é¢ä¸Šæ¥çœ‹ï¼Œæœºå™¨å­¦ä¹ æ˜¯ä¸€äº›åˆ†ç±»å’Œèšç±»ç®—æ³•ã€‚åœ¨è¿™äº›ç®—æ³•ä¸­ï¼Œä»‹ç»äº†ä¸€ç§ç®—æ³•ï¼Œå«åšé€»è¾‘å›å½’åˆ†ç±»ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å°èŠ‚9-12ä¸­ï¼Œä¸»è¦ä»‹ç»äº†åˆ†ç±»å™¨æ¨¡å‹â€”â€”é€»è¾‘å›å½’ï¼Œåˆ†ç±»å‡½æ•°ä½¿ç”¨çš„æ˜¯softmaxå‡½æ•°ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä»€ä¹ˆæ˜¯softmaxå‡½æ•°&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-81791b221c81e509.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;softmax&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;è¿™å¼ å›¾ç‰‡å¯ä»¥è¡¨æ˜ä»€ä¹ˆæ˜¯softmaxå‡½æ•°äº†ã€‚å¯¹åŸæ¥æ•°åˆ—ä¸­çš„æ¯ä¸ªæ•°zæ±‚exp(z)ï¼Œæ–°æ•°çš„å¤§å°æ‰€å çš„æ¯”ä¾‹å°±æ˜¯æ–°æ•°çš„softmaxæ¦‚ç‡ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ€§è´¨&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦‚æœè¾“å…¥åŒæ¯”ä¾‹æ‰©å¤§ï¼Œåˆ™åˆ†ç±»å™¨çš„ç»“æœè¶Šä¸¤æåŒ–ï¼Œè¶Šè‡ªä¿¡ï¼Œå¦‚æœè¾“å…¥åŒæ¯”ä¾‹ç¼©å°ï¼Œåˆ†ç±»å™¨ç»“æœè¶‹äºå¹³å‡ï¼Œä¸è‡ªä¿¡ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç®—æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;softmax&lt;/span&gt;(x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Compute softmax values for each sets of scores in x.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    expList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(i) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; x]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    expSum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sum(expList)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;expSum &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; expList]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(x)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-ee072532fdf320cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;13-14èŠ‚ä¸»è¦è®²One-Hotç¼–ç ã€‚åœ¨softmaxå‡½æ•°ç»™å‡ºä¸€ç»„æ¦‚ç‡æ•°åˆ—ä¹‹åï¼Œå¦‚ä½•ç¡®å®šåˆ†ç±»å‘¢ï¼Ÿä¾‹å¦‚æ¦‚ç‡æœ€é«˜çš„ä¸º1ï¼Œå…¶ä»–çš„ä¸º0ï¼Œè¿™æ ·çš„ä¸€ä¸ªæ•°åˆ—ï¼Œå±äºOne-Hotç¼–ç ã€‚è¿™ç§ç¼–ç æ˜¯å·²ç»ç¡®å®šäº†åˆ†ç±»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-d17a8063ae1224a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;äº¤å‰ç†µ&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;15-16èŠ‚è®²äº†äº¤å‰ç†µã€‚softmaxå¯ä»¥è®¡ç®—æ¦‚ç‡æ•°åˆ—ï¼ŒOneHotæ˜¯å·²ç»ç¡®å®šçš„åˆ†ç±»ï¼Œé‚£å¦‚ä½•è®¡ç®—æ¦‚ç‡æ•°åˆ—åˆ°æŸä¸ªåˆ†ç±»çš„è·ç¦»å‘¢ï¼Ÿä½¿ç”¨äº¤å‰ç†µæ¥åº¦é‡è¿™ä¸ªè·ç¦»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-17cca85ebc74d0ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-f28d86a84703d49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;17-20 èŠ‚è®²è§£äº†å¦‚ä½•ä½¿ç”¨è¿™ä¸ªåˆ†ç±»å™¨ã€‚å…¶ä¸­ï¼Œ18èŠ‚è®²äº†ä¸ºä»€ä¹ˆéœ€è¦é‡‡ç”¨ä¸€ç§ç‰¹æ®Šçš„åˆå§‹æ•°æ®ã€‚&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sum = 1000000000

for i in range(1000000):
    sum += 0.000001

sum -= 1000000000
print(sum)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;è¿™æ®µä»£ç è¿è¡Œç»“æœä¸æ˜¯1ã€‚å¦‚æœæŠŠsumæ¢æˆä¸€ä¸ªå¾ˆå°çš„æ•°å­—ï¼Œä¾‹å¦‚1ï¼Œè€Œä¸æ˜¯1000000000ï¼Œæˆ‘ä»¬å‘ç°ç»“æœè¯¯å·®å˜å°äº†ã€‚åŸºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬å¸Œæœ›åˆå§‹æ•°æ®æ€»æ˜¯å‡å€¼ä¸º0ï¼Œå¹¶ä¸”å„ä¸ªæ–¹å‘çš„æ–¹å·®ä¸ºä¸€è‡´çš„ã€‚ä¾‹å¦‚ä¸€ä¸ªç°åº¦å›¾ç‰‡çš„åƒç´ å€¼0-255ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒå‡å»128ï¼Œç„¶åé™¤ä»¥128ï¼Œè¿™æ ·æ¯ä¸€ä¸ªæ•°å­—éƒ½æ˜¯-1åˆ°1ä¹‹é—´çš„æ•°å­—ï¼Œè¿™æ ·çš„åˆå§‹æ•°æ®æ›´é€‚åˆç”¨æ¥è®­ç»ƒã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-5e75673ff68468cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œè®­ç»ƒäº†ã€‚å›é¡¾ä¸€ä¸‹è§†é¢‘å†…å®¹ï¼Œxiæ˜¯è®­ç»ƒæ•°æ®çŸ©é˜µï¼Œwæ˜¯éšæœºæƒé‡çŸ©é˜µï¼Œä¸ºäº†æ€§èƒ½ï¼Œéšæœºå€¼å–è‡ªæ­£æ€åˆ†å¸ƒä¸­è½´ä¸º0ï¼Œæ–¹å·®å¾ˆå°çš„åˆ†å¸ƒå‡½æ•°ï¼Œç„¶åè®¡ç®—æ¦‚ç‡æ•°åˆ—ï¼Œå’Œç›®æ ‡çš„è·ç¦»ã€‚ç„¶åæ±‚å‡ºåˆ°æ‰€æœ‰ç›®æ ‡çš„å¹³å‡è·ç¦»ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯è®©è·ç¦»å˜å°ï¼Œæ‰€ä»¥æˆ‘ä»¬æ²¿ç€æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ä¼˜åŒ–æƒé‡çŸ©é˜µï¼ŒåŒæ—¶ä¼˜åŒ–æˆªè·bã€‚ä¸æ–­é‡å¤è¿™ä¸€ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°å±€éƒ¨æœ€ä¼˜ä¸ºæ­¢ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å®‰è£…docker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.docker-cn.com/community-edition#/download&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.docker-cn.com/community-edition#/download&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;é…ç½®å®˜æ–¹ä¸­å›½é•œåƒã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-5f5ea990dda40440.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å®‰è£…jupyter notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;$ pip3 install jupyter&lt;/code&gt;
&lt;code&gt;$ jupyter notebook&lt;/code&gt;
æ­¤æ—¶å¯ä»¥ä½¿ç”¨å‘½ä»¤jupyter notebookæ‰“å¼€ä¸€ä¸ªjupyterç¼–è¾‘å™¨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tensorflowç¯å¢ƒæ­å»º&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;$ docker run -it -p 8888:8888 tensorflow/tensorflow&lt;/code&gt;
è¿è¡Œä¸Šè¿°å‘½ä»¤ä¼šè‡ªåŠ¨ä¸‹è½½tensorflowé•œåƒï¼Œå‰ææ˜¯ä»“åº“é•œåƒè®¾ç½®æˆä¸­å›½çš„é•œåƒï¼Œå¦åˆ™ä¸‹è½½å¾ˆæ…¢ã€‚è¿è¡Œå‘½ä»¤åï¼Œä¼šæç¤ºä½ æ‰“å¼€ç½‘é¡µï¼Œæ‰“å¼€è¿™ä¸ªç½‘å€ä»¥åä¼šæ˜¾ç¤ºtensorflowçš„jupyterç¼–è¾‘ç¯å¢ƒï¼Œå‰ææ˜¯jupyter notebookå®‰è£…æ­£ç¡®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŒ‚è½½dockerçš„æ–‡ä»¶ç›®å½•
æˆ‘ä»¬éœ€è¦æŠŠå®˜æ–¹çš„ä½œä¸šå¯¼è¿›å»ã€‚å…³é—­å®¹å™¨ï¼Œé‡æ–°æ‰“å¼€å®¹å™¨ï¼Œä½¿ç”¨&lt;code&gt;-v ä¸»æœºç›®å½•:å®¹å™¨ç›®å½•&lt;/code&gt;æ¥è¿›è¡ŒæŒ‚è½½ã€‚
&lt;code&gt;docker run -v /Users/hahaha/tensorflow/:/notebooks -it -p 8888:8888 tensorflow/tensorflow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å…¶ä¸­/Users/hahaha/tensorflow/æ˜¯æˆ‘çš„macçš„ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œnotebooksæ˜¯tensorflowä¸­çš„jupyteré»˜è®¤ç¼–è¾‘ç›®å½•ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ä¸»æœºç›®å½•çš„æŒ‚è½½ç›®å½•ä¸‹é¢ç²˜è´´ç¬¬ä¸€ä¸ªä½œä¸šæ–‡ä»¶ï¼Œ1_notmnist.ipynbã€‚è¿™ä¸ªæ–‡ä»¶å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š &lt;a class=&#34;link&#34; href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;1_notmnist.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-1a87bebfcc977690.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ä½œä¸šä¸€å†…å®¹&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ä½œä¸šä»£ç æ®µä¸€&#34;&gt;ä½œä¸šä»£ç æ®µä¸€
&lt;/h2&gt;&lt;p&gt;é¦–å…ˆè¿è¡Œä¸€ä¸‹ç¬¬ä¸€æ®µä»£ç çš„importï¼Œåº”è¯¥æ˜¯æ²¡æœ‰ä»»ä½•å‡ºé”™çš„ï¼Œæ­¤æ—¶ä»€ä¹ˆä¹Ÿä¸ä¼šå‘ç”Ÿï¼Œå¦‚æœå‡ºç°äº†çº¢è‰²çš„è¾“å‡ºé”™è¯¯ï¼Œé‚£å°±è¯´æ˜è¿™äº›from importæ²¡æœ‰å¯¼å…¥æˆåŠŸã€‚&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# These are all the modules we&amp;#39;ll be using later. Make sure you can import them
# before proceeding further.
from __future__ import print_function
# printå‡½æ•°
import matplotlib.pyplot as plt
# ç»˜å›¾å·¥å…·
import numpy as np
# çŸ©é˜µè®¡ç®—
import os
# æ–‡ä»¶è·¯å¾„
import sys
# æ–‡ä»¶è¾“å‡º
import tarfile
# è§£å‹ç¼©
from IPython.display import display, Image
# æ˜¾ç¤ºå›¾ç‰‡
from scipy import ndimage
# å›¾åƒå¤„ç†
from sklearn.linear_model import LogisticRegression
# é€»è¾‘å›å½’æ¨¡å—çº¿æ€§æ¨¡å‹
from six.moves.urllib.request import urlretrieve
# urlå¤„ç†
from six.moves import cPickle as pickle
# æ•°æ®å¤„ç†

# Config the matplotlib backend as plotting inline in IPython
%matplotlib inline
# matplotlibæ˜¯æœ€è‘—åçš„Pythonå›¾è¡¨ç»˜åˆ¶æ‰©å±•åº“ï¼Œ
# å®ƒæ”¯æŒè¾“å‡ºå¤šç§æ ¼å¼çš„å›¾å½¢å›¾åƒï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤šç§GUIç•Œé¢åº“äº¤äº’å¼åœ°æ˜¾ç¤ºå›¾è¡¨ã€‚
# ä½¿ç”¨%matplotlibå‘½ä»¤å¯ä»¥å°†matplotlibçš„å›¾è¡¨ç›´æ¥åµŒå…¥åˆ°Notebookä¹‹ä¸­ï¼Œ
# æˆ–è€…ä½¿ç”¨æŒ‡å®šçš„ç•Œé¢åº“æ˜¾ç¤ºå›¾è¡¨ï¼Œå®ƒæœ‰ä¸€ä¸ªå‚æ•°æŒ‡å®šmatplotlibå›¾è¡¨çš„æ˜¾ç¤ºæ–¹å¼ã€‚
# inlineè¡¨ç¤ºå°†å›¾è¡¨åµŒå…¥åˆ°Notebookä¸­ã€‚
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ä½œä¸šä»£ç æ®µäºŒ&#34;&gt;ä½œä¸šä»£ç æ®µäºŒ
&lt;/h2&gt;&lt;p&gt;æ¥ä¸‹æ¥æ˜¯ç¬¬äºŒæ®µä»£ç ï¼Œä¼šè¿›è¡Œä¸‹è½½ç”¨äºè®­ç»ƒå’Œæµ‹è¯•çš„å­—æ¯é›†åˆï¼Œå¤§æ¦‚æ˜¯300mbå¤§å°ã€‚ä¸‹è½½æˆåŠŸåï¼Œå¯ä»¥çœ‹åˆ°æŒ‚è½½ç›®å½•ä¸‹é¢çš„è¿™ä¸¤ä¸ªæ–‡ä»¶ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-e1cc51d654c2800a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;ä½œä¸š&#34;
	
	
&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;url = &amp;#39;https://commondatastorage.googleapis.com/books1000/&amp;#39;
last_percent_reported = None
data_root = &amp;#39;.&amp;#39; # Change me to store data elsewhere

def download_progress_hook(count, blockSize, totalSize):
  &amp;#34;&amp;#34;&amp;#34;A hook to report the progress of a download. This is mostly intended for users with
  slow internet connections. Reports every 5% change in download progress.
  &amp;#34;&amp;#34;&amp;#34;
# é’©å­å‡½æ•°ç”¨æ¥å®æ—¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
  global last_percent_reported
  percent = int(count * blockSize * 100 / totalSize)

  if last_percent_reported != percent:
    if percent % 5 == 0:
      sys.stdout.write(&amp;#34;%s%%&amp;#34; % percent)
      sys.stdout.flush()
    else:
      sys.stdout.write(&amp;#34;.&amp;#34;)
      sys.stdout.flush()
      
    last_percent_reported = percent
        
def maybe_download(filename, expected_bytes, force=False):
  &amp;#34;&amp;#34;&amp;#34;Download a file if not present, and make sure it&amp;#39;s the right size.&amp;#34;&amp;#34;&amp;#34;
  dest_filename = os.path.join(data_root, filename)
#   data_rootæ˜¯å½“å‰ç›®å½•ï¼Œåœ¨è¿™ä¸ªç›®å½•ä¸ŠåŠ ä¸Šæ–‡ä»¶åï¼Œè®¾ç½®ä¸ºè¦ä¿å­˜çš„æ–‡ä»¶ä½ç½®
  if force or not os.path.exists(dest_filename):
#         forceæ˜¯å¼ºåˆ¶ä¸‹è½½ï¼Œå¿½ç•¥å·²ç»ä¸‹è½½çš„æ–‡ä»¶
    print(&amp;#39;Attempting to download:&amp;#39;, filename) 
    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)
#     ä½¿ç”¨urlretrieveæ¥ä¸‹è½½æ–‡ä»¶ï¼ŒæŒ‚ä¸Šé’©å­
    print(&amp;#39;\nDownload Complete!&amp;#39;)
  statinfo = os.stat(dest_filename)
# è·å–ä¸‹è½½åˆ°çš„æ–‡ä»¶çš„ä¿¡æ¯
  if statinfo.st_size == expected_bytes:
#         æ­£ç¡®å¤§å°
    print(&amp;#39;Found and verified&amp;#39;, dest_filename)
  else:
#     é”™è¯¯å¤§å°ï¼Œæç¤ºç”¨æˆ·ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½
    raise Exception(
      &amp;#39;Failed to verify &amp;#39; + dest_filename + &amp;#39;. Can you get to it with a browser?&amp;#39;)
  return dest_filename

train_filename = maybe_download(&amp;#39;notMNIST_large.tar.gz&amp;#39;, 247336696)
test_filename = maybe_download(&amp;#39;notMNIST_small.tar.gz&amp;#39;, 8458043)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ä½œä¸šä»£ç æ®µä¸‰&#34;&gt;ä½œä¸šä»£ç æ®µä¸‰
&lt;/h2&gt;&lt;p&gt;è§£å‹ç¼©ç”¨ä¾‹&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;num_classes = 10
# æ•°å­—æ€»å…±æœ‰å¤šå°‘ä¸ª
np.random.seed(133)
# åˆå§‹åŒ–éšæœºç§å­
def maybe_extract(filename, force=False):
#     å‡è®¾å·²ç»è§£å‹ç¼©äº†
  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz
#     splitext(filename)[0]ç”¨äºå»é™¤ä¸€ä¸ªåç¼€ï¼Œç”¨ä¸¤æ¬¡å°±æ˜¯å»é™¤ä¸¤æ¬¡åç¼€ï¼Œä¹Ÿå°±æ˜¯å»é™¤.tar.gzè¿™ä¸ªåç¼€
  if os.path.isdir(root) and not force:
    # You may override by setting force=True.
#     å·²ç»è§£å‹ç¼©äº†å°±ä¸å†è§£å‹ç¼©äº†
    print(&amp;#39;%s already present - Skipping extraction of %s.&amp;#39; % (root, filename))
  else:
    print(&amp;#39;Extracting data for %s. This may take a while. Please wait.&amp;#39; % root)
    tar = tarfile.open(filename)
    sys.stdout.flush()
    tar.extractall(data_root)
    tar.close()
#     è§£å‹ç¼©åˆ°å½“å‰ç›®å½•ä¸‹é¢
  data_folders = [
    os.path.join(root, d) for d in sorted(os.listdir(root))
    if os.path.isdir(os.path.join(root, d))]
  if len(data_folders) != num_classes:
    raise Exception(
      &amp;#39;Expected %d folders, one per class. Found %d instead.&amp;#39; % (
        num_classes, len(data_folders)))
  print(data_folders)
# æ£€æŸ¥è§£å‹ç¼©æ–‡ä»¶ç›®å½•çš„æ•°é‡ä¸æœŸå¾…æ˜¯å¦ä¸€è‡´ï¼Œå¹¶ä¸”æ‰“å°è§£å‹ç¼©å‡ºæ¥æ–‡ä»¶çš„ç›®å½•
  return data_folders
  
train_folders = maybe_extract(train_filename)
test_folders = maybe_extract(test_filename)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;é—®é¢˜ä¸€&#34;&gt;é—®é¢˜ä¸€
&lt;/h2&gt;&lt;p&gt;å†™å‡ºä»£ç æ˜¾ç¤ºè§£å‹ç¼©çš„æ–‡ä»¶å†…å®¹ä¿¡æ¯&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‚è€ƒç­”æ¡ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import random
import matplotlib.image as mpimg


def plot_samples(data_folders, sample_size, title=None):
    fig = plt.figure()
#     å»ºç«‹ç©ºå›¾åƒ
    if title: fig.suptitle(title, fontsize=16, fontweight=&amp;#39;bold&amp;#39;)
#         åŠ å…¥æ ‡é¢˜
    for folder in data_folders:
#         éå†æ¯ä¸ªå­—æ¯
        image_files = os.listdir(folder)
        image_sample = random.sample(image_files, sample_size)
#         ä»è¯¥å­—æ¯ä¸­éšæœºé€‰å–ä¸€å®šæ•°é‡çš„å›¾ç‰‡
        for image in image_sample:
            image_file = os.path.join(folder, image)
            ax = fig.add_subplot(len(data_folders), sample_size, sample_size * data_folders.index(folder) +
                                 image_sample.index(image) + 1)
#             åˆ›å»ºä¸€ä¸ªå­å›¾
            image = mpimg.imread(image_file)
#     åŠ è½½å­å›¾å›¾ç‰‡
            ax.imshow(image)
#     æ˜¾ç¤ºå­å›¾å›¾ç‰‡
            ax.set_axis_off() 
#     å…³é—­å­å›¾åæ ‡çº¿

    fig.set_size_inches(18.5, 10.5)
#     è®¾ç½®å›¾ç‰‡æ˜¾ç¤ºçš„å¤§å°
    plt.show()


plot_samples(train_folders, 20, &amp;#39;Train&amp;#39;)
plot_samples(test_folders, 20, &amp;#39;Test&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;è¿è¡Œæ•ˆæœå¦‚ä¸‹&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-89f6aa390dfd06a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;è®­ç»ƒ.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-0dbfe7c00c15e9e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;æµ‹è¯•.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥çœ‹å‡ºï¼Œéƒ¨åˆ†è®­ç»ƒæ•°æ®æ˜¯æœ‰é—®é¢˜çš„&lt;/p&gt;
&lt;h2 id=&#34;ä½œä¸šä»£ç æ®µå››&#34;&gt;ä½œä¸šä»£ç æ®µå››
&lt;/h2&gt;&lt;p&gt;è¿™ä¹‹åéœ€è¦è¿›è¡Œæ•°æ®çš„å½’ä¸€åŒ–å¤„ç†ï¼Œå°±æ˜¯è®©å›¾åƒçš„æ¯ä¸€ä¸ªåƒç´ ç”±0&lt;del&gt;255å˜æ¢åˆ°-1.0&lt;/del&gt;1.0ï¼Œå¹¶ä¸”æŒä¹…åŒ–åˆ°æ–‡ä»¶ä¸­&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;image_size = 28  # Pixel width and height.
pixel_depth = 255.0  # Number of levels per pixel.
# å›¾ç‰‡é•¿å®½å’Œå›¾ç‰‡åƒç´ æ·±åº¦
def load_letter(folder, min_num_images):
  &amp;#34;&amp;#34;&amp;#34;Load the data for a single letter label.&amp;#34;&amp;#34;&amp;#34;
# å¤„ç†ä¸€ä¸ªå±äºä¸€ä¸ªå­—æ¯æ–‡ä»¶å¤¹ä¸‹é¢çš„æ–‡ä»¶
  image_files = os.listdir(folder)
#     åˆ—å‡ºè¯¥æ–‡ä»¶å¤¹ç›®å½•ä¸‹é¢çš„æ‰€æœ‰æ–‡ä»¶
  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
                         dtype=np.float32)
#     åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºæ–‡ä»¶ä¸ªæ•°ï¼Œå®½åº¦å’Œé«˜åº¦ä¸º28çš„
    
  print(folder)
# æ‰“å°ç›®å½•
  num_images = 0
# åˆå§‹åŒ–num_images
  for image in image_files:
#   å¯¹æ¯ä¸€ä¸ªæ–‡ä»¶å¤„ç†
    image_file = os.path.join(folder, image)
#     è·å–å®Œæ•´æ–‡ä»¶è·¯å¾„
    try:
      image_data = (ndimage.imread(image_file).astype(float) - 
                    pixel_depth / 2) / pixel_depth
#     è¯»å…¥å›¾åƒï¼Œå¹¶ä¸”å½’ä¸€åŒ–å¤„ç†
      if image_data.shape != (image_size, image_size):
#         æ£€æŸ¥å›¾åƒçš„å®½é«˜
        raise Exception(&amp;#39;Unexpected image shape: %s&amp;#39; % str(image_data.shape))
      dataset[num_images, :, :] = image_data
#         è¯»å…¥åˆ°æ•°æ®é›†åˆä¸­
      num_images = num_images + 1
#     å›¾ç‰‡åºå·åŠ ä¸€
    except IOError as e:
#         å¦‚æœæ— æ³•è¯»å–æ–‡ä»¶çš„è¯ï¼Œåˆ™å¿½ç•¥è¯¥æ–‡ä»¶
      print(&amp;#39;Could not read:&amp;#39;, image_file, &amp;#39;:&amp;#39;, e, &amp;#39;- it\&amp;#39;s ok, skipping.&amp;#39;)
    
  dataset = dataset[0:num_images, :, :]
# å¦‚æœè¯»è¿›æ¥çš„æ–‡ä»¶æ•°é‡å°‘äºæœ€å°éœ€è¦æ–‡ä»¶æ•°é‡
  if num_images &amp;lt; min_num_images:
    raise Exception(&amp;#39;Many fewer images than expected: %d &amp;lt; %d&amp;#39; %
                    (num_images, min_num_images))
#     æ˜¾ç¤ºç¼ºå°‘çš„æ–‡ä»¶æ•°é‡
  print(&amp;#39;Full dataset tensor:&amp;#39;, dataset.shape)
#     æ˜¾ç¤ºæ–‡ä»¶æ•°é‡ï¼Œå›¾ç‰‡é•¿å®½
  print(&amp;#39;Mean:&amp;#39;, np.mean(dataset))
#     å¹³å‡å€¼
  print(&amp;#39;Standard deviation:&amp;#39;, np.std(dataset))
#     æ ‡å‡†å·®
  return dataset
        
def maybe_pickle(data_folders, min_num_images_per_class, force=False):
  dataset_names = []
  for folder in data_folders:
#         å¯¹æ¯ä¸€ä¸ªå­—æ¯æ–‡ä»¶å¤¹å¤„ç†
    set_filename = folder + &amp;#39;.pickle&amp;#39;
#     è®¾ç½®è¾“å‡ºçš„æ–‡ä»¶
    dataset_names.append(set_filename)
#     è®¾ç½®å¤„ç†è¿‡çš„æ–‡ä»¶å¤¹
    if os.path.exists(set_filename) and not force:
      # You may override by setting force=True.
#     æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²å¤„ç†è¿‡çš„æ–‡ä»¶
      print(&amp;#39;%s already present - Skipping pickling.&amp;#39; % set_filename)
    else:
      print(&amp;#39;Pickling %s.&amp;#39; % set_filename)
      dataset = load_letter(folder, min_num_images_per_class)
#         å½’ä¸€åŒ–å¤„ç†è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢çš„æ‰€æœ‰å›¾ç‰‡
      try:
        with open(set_filename, &amp;#39;wb&amp;#39;) as f:
          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
#         æŒä¹…åŒ–æ•°æ®ï¼Œå°†æ•°æ®ä¿å­˜åœ¨ç¡¬ç›˜ä¸Šï¼Œè€Œä¸æ˜¯ä¸€ç›´æ”¾åœ¨å†…å­˜ä¸­
      except Exception as e:
        print(&amp;#39;Unable to save data to&amp;#39;, set_filename, &amp;#39;:&amp;#39;, e)
  
  return dataset_names

train_datasets = maybe_pickle(train_folders, 45000)
test_datasets = maybe_pickle(test_folders, 1800)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;é—®é¢˜2&#34;&gt;é—®é¢˜2
&lt;/h2&gt;&lt;p&gt;æ˜¾ç¤ºå¤„ç†è¿‡çš„å›¾ç‰‡&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‚è€ƒç­”æ¡ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def plot_samples_2(data_folders, sample_size, title=None):
    fig = plt.figure()
#     å»ºç«‹ç©ºå›¾åƒ
    if title: fig.suptitle(title, fontsize=16, fontweight=&amp;#39;bold&amp;#39;)
#         åŠ å…¥æ ‡é¢˜
    for folder in data_folders:
#         éå†æ¯ä¸ªå­—æ¯
        with open(folder, &amp;#39;rb&amp;#39;) as pk_f:
            data = pickle.load(pk_f)
            for index, image in enumerate(data):
                if index &amp;lt; sample_size :
#         ä»è¯¥å­—æ¯ä¸­éšæœºé€‰å–ä¸€å®šæ•°é‡çš„å›¾ç‰‡
                    ax = fig.add_subplot(len(data_folders), sample_size, sample_size * data_folders.index(folder) +
                                 index + 1)
#     åŠ è½½å­å›¾å›¾ç‰‡
                    ax.imshow(image)
#     æ˜¾ç¤ºå­å›¾å›¾ç‰‡
                    ax.set_axis_off() 
#     å…³é—­å­å›¾åæ ‡çº¿

    fig.set_size_inches(18.5, 10.5)
#     è®¾ç½®å›¾ç‰‡æ˜¾ç¤ºçš„å¤§å°
    plt.show()
    

plot_samples_2(train_datasets, 20, &amp;#39;Train&amp;#39;)
plot_samples_2(test_datasets, 20, &amp;#39;Test&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-e3406390a28cd9b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-135416c384df602a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image.png&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;é—®é¢˜3&#34;&gt;é—®é¢˜3
&lt;/h2&gt;&lt;p&gt;æ£€æŸ¥æ¯ä¸ªå­—æ¯ä¸‹é¢çš„æ–‡ä»¶æ•°ç›®æ˜¯å¦ç›¸ä¼¼ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‚è€ƒç­”æ¡ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;file_path = &amp;#39;notMNIST_large/{0}.pickle&amp;#39;
for ele in &amp;#39;ABCDEFJHIJ&amp;#39;:
    with open(file_path.format(ele), &amp;#39;rb&amp;#39;) as pk_f:
#         éå†æ¯ä¸€ä¸ªç›®å½•
        dat = pickle.load(pk_f)
#     åŠ è½½è¿™ä¸ªç›®å½•ä¸‹é¢çš„æŒä¹…åŒ–æ–‡ä»¶
    print(&amp;#39;number of pictures in {}.pickle = &amp;#39;.format(ele), dat.shape[0])
#     æ‰“å°ç›¸å…³ä¿¡æ¯
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ç»“æœè¡¨æ˜æ•°ç›®åŸºæœ¬ä¸€è‡´ã€‚
&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-dbeceed47af0c6d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;é—®é¢˜3æ•ˆæœ&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ä»£ç æ®µæ•°æ®åˆ†å‰²&#34;&gt;ä»£ç æ®µâ€”â€”æ•°æ®åˆ†å‰²
&lt;/h2&gt;&lt;p&gt;æ•°æ®ä¸å¯èƒ½ä¸€æ¬¡æ€§å°±å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè¿™é‡Œå¯¹è¿™äº›æ•°æ®è¿›è¡Œåˆ†å‰²ï¼Œæ¥ä¸‹æ¥çš„è¿™ä»½ä»£ç å¯¹æ•°æ®è¿›è¡Œäº†åˆ†å‰²&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def make_arrays(nb_rows, img_size):
  if nb_rows:
    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)
#     åˆ›å»ºä¸€ä¸ªç©ºé›†åˆï¼Œæ•°æ®ç±»å‹æ˜¯é•¿rowså®½img_sizeé«˜img_sizeçš„çŸ©é˜µï¼Œæ•°æ®ç±»å‹æ˜¯æµ®ç‚¹32ä½
    labels = np.ndarray(nb_rows, dtype=np.int32)
#     åˆ›å»ºä¸€ä¸ªæ ‡ç­¾ï¼Œæ•°æ®ç±»å‹æ˜¯32ä½æ•´å‹ï¼Œé•¿åº¦æ˜¯rows
  else:
    dataset, labels = None, None
  return dataset, labels
# è¿”å›åˆ›å»ºçš„æ•°æ®ç±»å‹

def merge_datasets(pickle_files, train_size, valid_size=0):
  num_classes = len(pickle_files)
#     éœ€è¦å¤„ç†çš„ç±»åˆ«æ•°é‡
  valid_dataset, valid_labels = make_arrays(valid_size, image_size)
#     å»ºç«‹æœ‰æ•ˆæ•°æ®é›†åˆï¼Œé•¿åº¦ä¸ºæœ‰æ•ˆé•¿åº¦
  train_dataset, train_labels = make_arrays(train_size, image_size)
#     å»ºç«‹è®­ç»ƒæ•°æ®é›†åˆï¼Œé•¿åº¦ä¸ºè®­ç»ƒé•¿åº¦
  vsize_per_class = valid_size // num_classes
  tsize_per_class = train_size // num_classes
# è®¡ç®—ç»™å®šè®­ç»ƒé•¿åº¦å’Œæœ‰æ•ˆé•¿åº¦ä¸‹æ¯ä¸ªç±»åˆ«çš„å¹³å‡é•¿åº¦

  start_v, start_t = 0, 0
# åˆå§‹åŒ–ä¸‹æ ‡ï¼Œstart_væ˜¯æœ‰æ•ˆæ•°æ®çš„å¼€å§‹ï¼Œstart_tæ˜¯è®­ç»ƒæ•°æ®çš„å¼€å§‹
  end_v, end_t = vsize_per_class, tsize_per_class
# åˆå§‹åŒ–ä¸‹æ ‡ï¼Œend_væ˜¯æœ‰æ•ˆæ•°æ®çš„ç»“æŸï¼Œend_tæ˜¯è®­ç»ƒæ•°æ®çš„ç»“æŸ
  end_l = vsize_per_class + tsize_per_class
# åˆå§‹åŒ–ä¸‹æ ‡ï¼Œend_læ˜¯å­—æ¯é›†åˆçš„ç»“æŸï¼Œç­‰äºæ¯ä¸ªç±»åˆ«æœ‰æ•ˆæ•°æ®çš„é•¿åº¦+è®­ç»ƒæ•°æ®çš„é•¿åº¦
  for label, pickle_file in enumerate(pickle_files):  
#         éå†æ¯ä¸€ä¸ªpickle_file
    try:
      with open(pickle_file, &amp;#39;rb&amp;#39;) as f:
#         æ‰“å¼€è¿™ä¸ªæŒä¹…åŒ–æ–‡ä»¶
        letter_set = pickle.load(f)
#       åŠ è½½æ•°æ®é›†
        # let&amp;#39;s shuffle the letters to have random validation and training set
        np.random.shuffle(letter_set)
#       æ‰“ä¹±æ•°æ®é›†çš„é¡ºåº
        if valid_dataset is not None:
#         å¦‚æœä¸æ˜¯æµ‹è¯•é›†çš„è¯ï¼Œæ›´æ–°æµ‹è¯•é›†ï¼Œå¦åˆ™ valid_dataset ä¸æ›´æ–°
          valid_letter = letter_set[:vsize_per_class, :, :]
#         numpyåˆ‡ç‰‡     http://brieflyx.me/2015/python-module/numpy-array-split/
#         ä»æ‰“ä¹±çš„æ•°æ®ä¸­é€‰æ‹© æ¯ç±»æœ‰æ•ˆæ•°æ® æ•°é‡çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ”¾åˆ° valid_letter ä¸­
          valid_dataset[start_v:end_v, :, :] = valid_letter
#         æŠŠè¿™ä»½æ•°æ®æ”¾åˆ°valid_datasetä¸­
          valid_labels[start_v:end_v] = label
#         æ ‡è®°label åº”è¯¥æ˜¯ 0~9ä¸­çš„ä¸€ç§
          start_v += vsize_per_class
          end_v += vsize_per_class
#         æ›´æ–°ä¸‹æ ‡
#       å¾ªç¯ç»“æŸæ—¶ï¼Œ valid_dataset åº”è¯¥æ€»é•¿åº¦ä¸º valid_size çš„ä¸€ä»½æ•°æ®ï¼Œ valid_labelsæ˜¯å¯¹åº”ä½ç½®çš„æ ‡ç­¾

        train_letter = letter_set[vsize_per_class:end_l, :, :]
#       é™¤å»validéƒ¨åˆ†çš„éšæœºå…¶ä»–å…ƒç´ ï¼Œé•¿åº¦ä¸º end_l - vsize_per_class = tsize_per_class
        train_dataset[start_t:end_t, :, :] = train_letter
#       å¾ªç¯ç»“æŸæ—¶ï¼Œtrain_datasetåº”è¯¥æ˜¯æ€»é•¿ä¸º train_size çš„ ä¸€ä»½æ•°æ®
        
#       
        train_labels[start_t:end_t] = label
        start_t += tsize_per_class
        end_t += tsize_per_class
#       æ›´æ–°ä¸‹æ ‡
    except Exception as e:
      print(&amp;#39;Unable to process data from&amp;#39;, pickle_file, &amp;#39;:&amp;#39;, e)
      raise
    
  return valid_dataset, valid_labels, train_dataset, train_labels
            
            
train_size = 200000
valid_size = 10000
test_size = 10000

valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(
  train_datasets, train_size, valid_size)
_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)

print(&amp;#39;Training:&amp;#39;, train_dataset.shape, train_labels.shape)
print(&amp;#39;Validation:&amp;#39;, valid_dataset.shape, valid_labels.shape)
print(&amp;#39;Testing:&amp;#39;, test_dataset.shape, test_labels.shape)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ä»£ç æ®µæ‰“æ•£æ•°æ®&#34;&gt;ä»£ç æ®µâ€”â€”æ‰“æ•£æ•°æ®
&lt;/h2&gt;&lt;p&gt;permutationå‡½æ•°ä»‹ç»ï¼šhttp://www.jianshu.com/p/f0eb10acaa2d&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def randomize(dataset, labels):
#     labels.shape[0] æ˜¯ labels çš„é•¿åº¦
  permutation = np.random.permutation(labels.shape[0])
#     éšæœºå–å‡ºè¿™ä¹ˆå¤šæ•°å­—çš„æ‰“ä¹±
  print(labels.shape[0])
  shuffled_dataset = dataset[permutation,:,:]
# æ‰“ä¹±æ•°æ®
  shuffled_labels = labels[permutation]
# æ‰“ä¹±æ ‡ç­¾
  return shuffled_dataset, shuffled_labels
train_dataset, train_labels = randomize(train_dataset, train_labels)
test_dataset, test_labels = randomize(test_dataset, test_labels)
valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;é—®é¢˜4&#34;&gt;é—®é¢˜4
&lt;/h2&gt;&lt;p&gt;æ£€éªŒæ‰“æ•£åçš„æ•°æ®æ˜¯å¦æ­£ç¡®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‚è€ƒç­”æ¡ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import random
def plot_sample_3(dataset, labels, title):
    fig = plt.figure()
    plt.suptitle(title, fontsize=16, fontweight=&amp;#39;bold&amp;#39;)
#     è®¾ç½®æ ‡é¢˜æ ·å¼
    items = random.sample(range(len(labels)), 200)
#     æ‰“æ•£ labels é•¿çš„é¡ºåºåºåˆ—
    for i, item in enumerate(items):
#         éšæœºå–ä¸€ä¸ª
        plt.subplot(10, 20, i + 1)
#     ç”»å­å›¾
        plt.axis(&amp;#39;off&amp;#39;)
#     å…³é—­åæ ‡è½´
        plt.title(chr(ord(&amp;#39;A&amp;#39;) + labels[item]))
#     åŠ æ ‡é¢˜
        plt.imshow(dataset[item])
#     æ˜¾ç¤ºå¯¹åº”ä½ç½®çš„å­å›¾
    fig.set_size_inches(18.5, 10.5)
    plt.show()
#     æ˜¾ç¤ºå›¾ç‰‡
 
plot_sample_3(train_dataset, train_labels, &amp;#39;train dataset suffled&amp;#39;)
plot_sample_3(valid_dataset, valid_labels, &amp;#39;valid dataset suffled&amp;#39;)
plot_sample_3(test_dataset, test_labels, &amp;#39;test dataset suffled&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/4388248-c33532945864acd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;é—®é¢˜4&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;çœç•¥ç±»ä¼¼çš„ä¸¤å›¾&lt;/p&gt;
&lt;h2 id=&#34;ä»£ç æ®µä¿å­˜æ•°æ®&#34;&gt;ä»£ç æ®µâ€”â€”ä¿å­˜æ•°æ®
&lt;/h2&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pickle_file = os.path.join(data_root, &amp;#39;notMNIST.pickle&amp;#39;)
# è¾“å‡ºæ–‡ä»¶è·¯å¾„
try:
  f = open(pickle_file, &amp;#39;wb&amp;#39;)
# æ‰“å¼€è¿™ä¸ªæ–‡ä»¶
  save = {
    &amp;#39;train_dataset&amp;#39;: train_dataset,
    &amp;#39;train_labels&amp;#39;: train_labels,
    &amp;#39;valid_dataset&amp;#39;: valid_dataset,
    &amp;#39;valid_labels&amp;#39;: valid_labels,
    &amp;#39;test_dataset&amp;#39;: test_dataset,
    &amp;#39;test_labels&amp;#39;: test_labels,
    }
#     å†™å…¥ä¸€ä¸ªå­—å…¸ string-ndarray
  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
  f.close()
except Exception as e:
  print(&amp;#39;Unable to save data to&amp;#39;, pickle_file, &amp;#39;:&amp;#39;, e)
  raise
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ä»£ç æ®µæ˜¾ç¤ºä¿å­˜æ•°æ®çš„å¤§å°&#34;&gt;ä»£ç æ®µâ€”â€”æ˜¾ç¤ºä¿å­˜æ•°æ®çš„å¤§å°
&lt;/h2&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;statinfo = os.stat(pickle_file)
print(&amp;#39;Compressed pickle size:&amp;#39;, statinfo.st_size)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;é—®é¢˜5&#34;&gt;é—®é¢˜5
&lt;/h2&gt;&lt;p&gt;é¢˜ç›®çš„Googleç¿»è¯‘&lt;/p&gt;
&lt;p&gt;é€šè¿‡æ„å»ºï¼Œæ­¤æ•°æ®é›†å¯èƒ½åŒ…å«å¤§é‡é‡å æ ·æœ¬ï¼ŒåŒ…æ‹¬éªŒè¯å’Œæµ‹è¯•é›†ä¸­ä¹ŸåŒ…å«çš„è®­ç»ƒæ•°æ®ï¼ è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„é‡å å¯èƒ½ä¼šä½¿ç»“æœåæ–œï¼Œå¦‚æœæ‚¨å¸Œæœ›åœ¨æ²¡æœ‰é‡å çš„ç¯å¢ƒä¸­ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼Œä½†å¦‚æœæ‚¨å¸Œæœ›åœ¨ä½¿ç”¨è®­ç»ƒæ ·æœ¬æ—¶å†æ¬¡çœ‹åˆ°è®­ç»ƒæ ·æœ¬ï¼Œé‚£ä¹ˆå®é™…ä¸Šæ˜¯å¯ä»¥çš„ã€‚ æµ‹é‡åŸ¹è®­ï¼ŒéªŒè¯å’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„é‡å ç¨‹åº¦ã€‚
å¯é€‰é—®é¢˜ï¼š
æ•°æ®é›†ä¹‹é—´çš„é‡å¤æ•°æ®æ€ä¹ˆæ ·ï¼Ÿ ï¼ˆå‡ ä¹ç›¸åŒçš„å›¾åƒï¼‰
åˆ›å»ºä¸€ä¸ªæ¶ˆæ¯’éªŒè¯å’Œæµ‹è¯•é›†ï¼Œå¹¶æ¯”è¾ƒæ‚¨åœ¨éšåçš„ä½œä¸šä¸­çš„å‡†ç¡®æ€§ã€‚&lt;/p&gt;
&lt;p&gt;å¤§æ¦‚æ„æ€æ˜¯è®­ç»ƒæ•°æ®ä¸èƒ½å’Œæµ‹è¯•ç”¨çš„æ•°æ®é‡åˆï¼Œå¦åˆ™å¯¼è‡´å‡†ç¡®åº¦ä¸å‡†&lt;/p&gt;
&lt;p&gt;å‚è€ƒä»£ç ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä»…ä»…æŸ¥çœ‹é‡å¤çš„å›¾ç‰‡æ•°é‡&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import hashlib

pickle_file = os.path.join(&amp;#39;.&amp;#39;, &amp;#39;notMNIST.pickle&amp;#39;)
try:
    with open(pickle_file, &amp;#39;rb&amp;#39;) as f:
        data = pickle.load(f)
except Exception as e:
  print(&amp;#39;Unable to open data from&amp;#39;, pickle_file, &amp;#39;:&amp;#39;, e)
  raise
# è‡ªä»ä¿å­˜æ•°æ®åï¼Œå¦‚æœkernelæŒ‚äº†ï¼Œå°±å¯ä»¥ä»æœ¬åœ°ç›´æ¥è¯»å–ï¼Œä¸ç”¨é‡æ–°è¿è¡Œä¹‹å‰çš„ä»£ç 
# å¦‚æœæŠ¥é”™çš„è¯ï¼Œå¯ä»¥åœ¨ç½‘ä¸Šæœç´¢æŠ¥é”™çš„å¼‚å¸¸

def calcOverlap(sourceSet, targetSet, description):
    sourceSetMd5 = np.array([hashlib.md5(img).hexdigest() for img in sourceSet])
#     å»ºç«‹ä¸€ä¸ªmd5è¡¨æ ¼
    targetSetMd5 = np.array([hashlib.md5(img).hexdigest() for img in targetSet])
#     å»ºç«‹ä¸€ä¸ªmd5è¡¨æ ¼
    overlap = np.intersect1d(sourceSetMd5, targetSetMd5, assume_unique=False)
#     å»é‡
    print(description)
    print(&amp;#34;overlap&amp;#34;,overlap.shape[0], &amp;#34;from&amp;#34;,sourceSetMd5.shape[0],&amp;#34;to&amp;#34;, targetSetMd5.shape[0])
    print(&amp;#34;rate&amp;#34;,overlap.shape[0]*100.0/sourceSetMd5.shape[0],&amp;#34;% and&amp;#34;, overlap.shape[0]*100.0/targetSetMd5.shape[0],&amp;#34;%&amp;#34;)
#     æ‰“å°é‡å æ•°é‡


calcOverlap(data[&amp;#39;train_dataset&amp;#39;], data[&amp;#39;valid_dataset&amp;#39;], &amp;#34;train_dataset &amp;amp; valid_dataset&amp;#34;)
calcOverlap(data[&amp;#39;train_dataset&amp;#39;], data[&amp;#39;test_dataset&amp;#39;], &amp;#34;train_dataset &amp;amp; test_dataset&amp;#34;)
calcOverlap(data[&amp;#39;test_dataset&amp;#39;], data[&amp;#39;valid_dataset&amp;#39;], &amp;#34;test_dataset &amp;amp; valid_dataset&amp;#34;)```

![è¿è¡Œæ•ˆæœ](http://upload-images.jianshu.io/upload_images/4388248-2882159fe68dc672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- å»é™¤é‡å¤å›¾ç‰‡èµ„æº
å¾…æ›´æ–°

## é—®é¢˜6
ä½¿ç”¨é€»è¾‘å›å½’è®­ç»ƒæ¨¡å‹å¹¶ä¸”è¿›è¡Œæµ‹è¯•

- å‚è€ƒä»£ç 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;import random
def disp_sample_dataset(dataset, labels,trueLabels, title=None):&lt;/p&gt;
&lt;h1 id=&#34;å±•ç¤ºè®­ç»ƒçš„ç»“æœ&#34;&gt;å±•ç¤ºè®­ç»ƒçš„ç»“æœ
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;fig = plt.figure()
if title: fig.suptitle(title, fontsize=16, fontweight=&#39;bold&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;è®¾ç½®æ ‡é¢˜æ ·å¼&#34;&gt;è®¾ç½®æ ‡é¢˜æ ·å¼
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;items = random.sample(range(len(labels)), 200)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;éšæœºé€‰æ‹©ä¸€ç³»åˆ—å›¾ç‰‡&#34;&gt;éšæœºé€‰æ‹©ä¸€ç³»åˆ—å›¾ç‰‡
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;for i, item in enumerate(items):
    plt.subplot(10, 20, i + 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;è®¾ç½®ä¸€ä¸ªå­å›¾&#34;&gt;è®¾ç½®ä¸€ä¸ªå­å›¾
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    plt.axis(&#39;off&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;å…³é—­åæ ‡çº¿&#34;&gt;å…³é—­åæ ‡çº¿
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    lab = str(chr(ord(&#39;A&#39;) + labels[item]))
    trueLab = str(chr(ord(&#39;A&#39;) + trueLabels[item]))
    if lab == trueLab:
        plt.title( lab )
    else:
        plt.title(lab + &amp;quot; but &amp;quot; + trueLab)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;åŠ ä¸Šæ ‡é¢˜&#34;&gt;åŠ ä¸Šæ ‡é¢˜
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;    plt.imshow(dataset[item])
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;æ˜¾ç¤ºè¿™ä¸ªå›¾ç‰‡&#34;&gt;æ˜¾ç¤ºè¿™ä¸ªå›¾ç‰‡
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;fig.set_size_inches(18.5, 10.5)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;def train_and_predict(train_dataset, train_labels, test_dataset, test_labels ,sample_size):
regr = LogisticRegression()&lt;/p&gt;
&lt;h1 id=&#34;ç”Ÿæˆè®­ç»ƒå™¨&#34;&gt;ç”Ÿæˆè®­ç»ƒå™¨
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;X_train = train_dataset[:sample_size].reshape(sample_size, 784)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;æ ¹æ®sample_sizeé€‰æ‹©è¦è®­ç»ƒçš„æ•°æ®é‡&#34;&gt;æ ¹æ®sample_sizeé€‰æ‹©è¦è®­ç»ƒçš„æ•°æ®é‡
&lt;/h1&gt;&lt;h1 id=&#34;æŠŠäºŒç»´å‘é‡å‹ç¼©åˆ°ä¸€ç»´å‘é‡&#34;&gt;æŠŠäºŒç»´å‘é‡å‹ç¼©åˆ°ä¸€ç»´å‘é‡
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;y_train = train_labels[:sample_size]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;å–å‡ºè®­ç»ƒæ•°æ®&#34;&gt;å–å‡ºè®­ç»ƒæ•°æ®
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;regr.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;è®­ç»ƒæ•°æ®&#34;&gt;è®­ç»ƒæ•°æ®
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;X_test = test_dataset.reshape(test_dataset.shape[0], 28 * 28)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;å°†æµ‹è¯•æ•°æ®å‹ç¼©åˆ°ä¸€ç»´å‘é‡&#34;&gt;å°†æµ‹è¯•æ•°æ®å‹ç¼©åˆ°ä¸€ç»´å‘é‡
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;y_test = test_labels
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;æµ‹è¯•æ•°æ®æ‰€å¯¹åº”çš„çœŸå®æ ‡ç­¾&#34;&gt;æµ‹è¯•æ•°æ®æ‰€å¯¹åº”çš„çœŸå®æ ‡ç­¾
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;pred_labels = regr.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;ç”Ÿæˆé¢„æµ‹æ•°æ®&#34;&gt;ç”Ÿæˆé¢„æµ‹æ•°æ®
&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;print(&#39;Accuracy:&#39;, regr.score(X_test, y_test), &#39;when sample_size=&#39;, sample_size)
disp_sample_dataset(test_dataset, pred_labels, test_labels, &#39;sample_size=&#39; + str(sample_size))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;train_and_predict(data[&amp;rsquo;train_dataset&amp;rsquo;],data[&amp;rsquo;train_labels&amp;rsquo;],data[&amp;rsquo;test_dataset&amp;rsquo;],data[&amp;rsquo;test_labels&amp;rsquo;], 1000)&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
![image.png](http://upload-images.jianshu.io/upload_images/4388248-6b3fb8a1d1b1ce34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## æ¨¡å‹æ€§èƒ½

å°èŠ‚22~27æåˆ°äº†æ¨¡å‹æ€§èƒ½çš„ç›¸å…³çŸ¥è¯†ã€‚æˆ‘ä»¬é€šå¸¸å¸Œæœ›æ¨¡å‹çš„æ€§èƒ½èƒ½å¤Ÿè¾¾åˆ°100%ï¼Œæ˜¾ç„¶æ˜¯ä¸å¯èƒ½çš„ã€‚å¹¶ä¸”ï¼Œä¸ºäº†ä½¿è®­ç»ƒé›†çš„å‡†ç¡®æ€§æé«˜ï¼Œæ¨¡å‹å¯èƒ½ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚è¿™æ—¶è¦éµå¾ªä¸¤ç‚¹ã€‚
- ä¸è¦å°†è®­ç»ƒæ•°æ®ä¸€æ¬¡æ€§ä½¿ç”¨ï¼Œè€Œæ˜¯åˆ†å—ä½¿ç”¨ï¼Œæ¯æ¬¡è®­ç»ƒä¸€éƒ¨åˆ†
- å½“æ¨¡å‹å‚æ•°ä½¿30ä¸ªä»¥ä¸Šçš„ç”¨ä¾‹ç”±é”™è¯¯å˜æˆæ­£ç¡®ï¼Œåˆ™è¿™ä¸ªå‚æ•°çš„æ”¹å˜æ˜¯æœ‰æ•ˆæœçš„ã€‚


![æ¨¡å‹æ€§èƒ½](http://upload-images.jianshu.io/upload_images/4388248-033910ba1d5c09e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## éšæœºæ¢¯åº¦ä¸‹é™
å°èŠ‚29~31è®²è§£äº†ä»€ä¹ˆæ˜¯éšæœºæ¢¯åº¦ä¸‹é™
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸ºäº†è®©æ¨¡å‹æœç€æœ€ä¼˜çš„æ–¹å‘èµ°ï¼Œéœ€è¦è®¡ç®—è¯¥ç‚¹çš„å¯¼æ•°ã€‚1.å¯¼æ•°çš„è®¡ç®—é‡æ¯”è¾ƒå¤§ï¼Œæˆ‘ä»¬éœ€è¦éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬æ¥è®¡ç®—å¯¼æ•°ï¼Œæ¥ä»£æ›¿çœŸå®çš„å¯¼æ•°ã€‚è¿™å°±æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ã€‚2.ä¸ºäº†å‡ç¼“éšæœºé€‰æ‹©çš„éšæœºæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ¨é‡çš„æƒ¯æ€§æ¥å‡å°‘éšæœºæ€§ã€‚3.ä¸ºäº†è®©åæœŸæ¨¡å‹èƒ½å¤Ÿç¨³å®šï¼Œæˆ‘ä»¬å‡å°‘å­¦ä¹ çš„æ­¥é•¿ã€‚

è¯¾ç¨‹ä¸€ç»“æŸ


&amp;gt; ä½œä¸šä»£ç å‚è€ƒ
&amp;gt; http://www.hankcs.com/ml/notmnist.html
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        
    </channel>
</rss>
